# ğŸ“ CS50x 2025 â€“ Lecture 3: Algorithms ğŸš€

This briefing document summarizes the key concepts and themes from **CS50x 2025 Lecture 3**.  
The lecture pivots away from introducing new C syntax to focus almost entirely on **the design, analysis, and implementation of algorithms**. It revisits the core idea that computer science is about **problem-solving efficiently**, showing how algorithmic design affects performance dramatically at scale.

---

## ğŸ§­ I. Introduction: The Importance of Good Design

The lecture revisits the **phone book problem** from Week 0 to re-emphasize that correctness alone is not enough â€” **efficiency matters**.

### Key Ideas
- **Correctness vs. Design:** A program can be correct but still inefficient. Good design makes solutions *fast* and *scalable*.
- **Phone Book Algorithms:**
  | Algorithm | Method | Complexity | Notes |
  |------------|---------|-------------|-------|
  | Linear Search (1 page) | Check one by one | O(n) | Correct but slow |
  | Linear Search (2 pages) | Check two at a time | O(n/2) â‰ˆ O(n) | Slightly faster, same scale |
  | Binary Search | Divide and conquer | O(log n) | Huge performance gain |

> â€œThis week is really going to be about implementing algorithms, not only correctly but also efficiently.â€

---

## âš™ï¸ II. Analyzing Algorithms: Asymptotic Notation

### ğŸ“ˆ Big O (O)
Describes the **worst-case** running time â€” the upper bound.  
Focuses on how performance scales as *n* grows.

- Linear Search â†’ **O(n)**
- Binary Search â†’ **O(log n)**

### ğŸ“‰ Big Î© (Omega)
Describes the **best-case** running time â€” the lower bound.

- Linear Search â†’ Î©(1) (if item is first)
- Binary Search â†’ Î©(1) (if item is in the middle)

### âš–ï¸ Big Î˜ (Theta)
Used when the best and worst cases grow at the same rate.

---

## ğŸ”¢ III. The Problem of Sorting

Efficient searching like binary search requires **sorted data**, introducing the **sorting problem**.

- **Need for Sorting:** Enables fast searching. Common in databases, contact lists, etc.
- **Trade-off:** Sorting takes time upfront but saves time when searching multiple times.

> â€œIf you expect to search often, itâ€™s worth keeping the data sorted.â€

---

## ğŸ§® IV. Two Simple Sorting Algorithms

### ğŸŸ¡ Selection Sort
**Idea:** Find the smallest element in the unsorted list and move it to the front.

```c
for (int i = 0; i < n - 1; i++) {
    int min = i;
    for (int j = i + 1; j < n; j++) {
        if (array[j] < array[min])
            min = j;
    }
    swap(array[i], array[min]);
}
```
- **Runtime:** Î˜(nÂ²)
- **Simple**, but inefficient for large data sets.

---

### ğŸ”µ Bubble Sort
**Idea:** Repeatedly compare and swap adjacent elements if theyâ€™re out of order.  
Each pass â€œbubblesâ€ the largest element to the end.

```c
for (int i = 0; i < n - 1; i++) {
    bool swapped = false;
    for (int j = 0; j < n - i - 1; j++) {
        if (array[j] > array[j + 1]) {
            swap(array[j], array[j + 1]);
            swapped = true;
        }
    }
    if (!swapped) break;
}
```
- **Worst-case:** Î˜(nÂ²)
- **Best-case:** Î©(n) (if no swaps needed)

---

## ğŸ§± V. Structs and Custom Data Types

To handle complex data (like names + numbers), C provides **structs**.

### ğŸ§© Problem
Parallel arrays (one for names, one for numbers) are error-prone.

### âœ… Solution: Structs
```c
typedef struct {
    string name;
    string number;
} person;
```
- **typedef** creates a custom data type `person`.
- Access fields with dot notation: `people[0].name`.

---

## ğŸ” VI. Recursion: A New Way of Thinking

### ğŸ’¡ Definition
A function is **recursive** if it **calls itself**.

### ğŸ§  Core Components
- **Base Case:** Stops the recursion.
- **Recursive Step:** Solves a smaller version of the problem.

**Example: Drawing a pyramid**
> To draw a pyramid of height `n`, first draw one of height `n - 1`, then add the bottom row.

---

## ğŸ§© VII. A Better Sorting Algorithm: Merge Sort

A powerful, **recursive** and much faster sorting method.

### ğŸ“œ Algorithm Steps
1. **Base Case:** If â‰¤ 1 element â†’ already sorted.
2. **Recursive Step:** Sort the left half, then the right half.
3. **Merge Step:** Combine the two sorted halves.

### ğŸ§® Merge Step
Compare the first elements of each sub-list, take the smaller, and repeat â€” O(n).

### âš™ï¸ Complexity
- **Time:** O(n log n)
- **Space:** Uses extra memory for merging.

### â³ Trade-off
Merge Sort is **much faster** than Î˜(nÂ²) sorts but uses more space.

---

## ğŸ¬ VIII. Conclusion: The Power of Algorithmic Design

The lecture ends with a **visual comparison** of sorting speeds â€” showing just how much faster Merge Sort is compared to Selection and Bubble Sort.

> A well-designed algorithm isnâ€™t just theory â€” itâ€™s the difference between a program that finishes in seconds and one that would take years.

---

ğŸ“˜ **Summary Table**

| Algorithm | Best Case | Worst Case | Avg. Case | Type |
|------------|------------|-------------|------------|------|
| Linear Search | Î©(1) | O(n) | Î˜(n) | Search |
| Binary Search | Î©(1) | O(log n) | Î˜(log n) | Search |
| Selection Sort | Î©(nÂ²) | O(nÂ²) | Î˜(nÂ²) | Sort |
| Bubble Sort | Î©(n) | O(nÂ²) | Î˜(nÂ²) | Sort |
| Merge Sort | Î©(n log n) | O(n log n) | Î˜(n log n) | Sort |

---

âœ¨ **Key Takeaway:**  
Algorithmic design defines the *efficiency* of your solution. The right algorithm can turn an impossible problem into a trivial one.
